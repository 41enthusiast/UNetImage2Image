/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
wandb: WARNING The anonymous setting has no effect and will be removed in a future version.
wandb: Currently logged in as: mridulav to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 1s6c6sg0
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/wandb/run-20251225_095318-1s6c6sg0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unet_efb0_730k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mridulav/unet_i2i
wandb: üöÄ View run at https://wandb.ai/mridulav/unet_i2i/runs/1s6c6sg0
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
  0%|          | 0.00/20.5M [00:00<?, ?B/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 13.8M/20.5M [00:00<00:00, 144MB/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 162MB/s]
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 train.py ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name  | Type                             | Params | Mode  | FLOPs
---------------------------------------------------------------------------
0 | model | UNet_pcb                         | 728 K  | train | 0    
1 | MSE   | MSELoss                          | 0      | train | 0    
2 | psnr  | PeakSignalNoiseRatio             | 0      | train | 0    
3 | ssim  | StructuralSimilarityIndexMeasure | 0      | train | 0    
---------------------------------------------------------------------------
726 K     Trainable params
2.4 K     Non-trainable params
728 K     Total params
2.916     Total estimated model params size (MB)
230       Modules in train mode
0         Modules in eval mode
0         Total Flops
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:485: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home3/txlx81/UNetImage2Image/uneti2i/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Metric val_loss improved. New best score: 0.058
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.056
Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.050
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.050
Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.032
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.031
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.028
Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.024
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.023
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.023
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.023
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.022
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.022
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.021
Monitored metric val_loss did not improve in the last 10 records. Best score: 0.021. Signaling Trainer to stop.
